{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Colab Training Template\n",
    "\n",
    "Optimized training notebook for Google Colab.\n",
    "\n",
    "**Before running:**\n",
    "1. Complete `00_Colab_Setup.ipynb` first\n",
    "2. Enable GPU runtime\n",
    "3. Ensure dataset is in Drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Drive and setup paths\n",
    "from google.colab import drive\n",
    "import os\n",
    "import sys\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Set paths\n",
    "PROJECT_ROOT = '/content/drive/MyDrive/endo-seg'\n",
    "CODE_DIR = '/content/endo-seg'\n",
    "\n",
    "# Create symlinks\n",
    "!mkdir -p {CODE_DIR}\n",
    "%cd {CODE_DIR}\n",
    "\n",
    "!ln -sf {PROJECT_ROOT}/data ./data\n",
    "!ln -sf {PROJECT_ROOT}/experiments ./experiments\n",
    "\n",
    "sys.path.append(f'{CODE_DIR}/src')\n",
    "\n",
    "print(\"\u2713 Setup complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify GPU\n",
    "import torch\n",
    "\n",
    "print(f\"GPU Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"GPU not available! Go to Runtime \u2192 Change runtime type \u2192 GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install missing packages (Colab has most pre-installed)\n",
    "!pip install -q monai nibabel SimpleITK\n",
    "!pip install -q pytorch-lightning\n",
    "!pip install -q wandb  # Optional: for experiment tracking\n",
    "\n",
    "print(\"\u2713 Dependencies installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration optimized for Colab T4\n",
    "config = {\n",
    "    # Paths\n",
    "    'data_root': f'{PROJECT_ROOT}/data/raw/UT-EndoMRI',\n",
    "    'splits_file': f'{PROJECT_ROOT}/data/splits/split_info.json',\n",
    "    'checkpoint_dir': f'{PROJECT_ROOT}/experiments/checkpoints',\n",
    "    'log_dir': f'{PROJECT_ROOT}/experiments/logs',\n",
    "    \n",
    "    # Data\n",
    "    'dataset_name': 'D2_TCPW',\n",
    "    'sequences': ['T2FS'],\n",
    "    'structures': ['uterus', 'ovary', 'endometrioma'],\n",
    "    \n",
    "    # Preprocessing (smaller size for T4 GPU)\n",
    "    'target_spacing': (5.0, 5.0, 5.0),\n",
    "    'target_size': (96, 96, 24),  # Smaller for Colab\n",
    "    \n",
    "    # Training (optimized for T4)\n",
    "    'batch_size': 2,  # Max for T4 with this size\n",
    "    'num_workers': 2,\n",
    "    'epochs': 100,\n",
    "    'learning_rate': 1e-4,\n",
    "    'weight_decay': 1e-5,\n",
    "    \n",
    "    # Save every N epochs (important for Colab!)\n",
    "    'save_frequency': 10,\n",
    "    \n",
    "    # Model\n",
    "    'model_name': 'unet',  # Start with simple U-Net\n",
    "    'num_classes': 4,  # Background + 3 structures\n",
    "    \n",
    "    # Optimization\n",
    "    'mixed_precision': True,  # Faster training\n",
    "    'gradient_clip': 1.0,\n",
    "}\n",
    "\n",
    "print(\"Configuration:\")\n",
    "for k, v in config.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train/val/test splits\n",
    "from endo_seg.data import load_data_splits\n",
    "\n",
    "splits = load_data_splits(config['splits_file'])\n",
    "\n",
    "print('Data splits:')\n",
    "print('  Train: {} subjects'.format(len(splits['train'])))\n",
    "print('  Val: {} subjects'.format(len(splits['val'])))\n",
    "print('  Test: {} subjects'.format(len(splits['test'])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create DataLoaders\n",
    "\n",
    "**Note:** This cell assumes you have your source files uploaded. If not, you'll need to copy the relevant code from your src/ directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build dataloaders using the packaged helpers\n",
    "from endo_seg.data import get_dataloaders\n",
    "\n",
    "sequences_cfg = {seq: True for seq in config['sequences']}\n",
    "structures_cfg = {struct: True for struct in config['structures']}\n",
    "\n",
    "dataloader_config = {\n",
    "    'sequences': sequences_cfg,\n",
    "    'structures': structures_cfg,\n",
    "    'preprocessing': {\n",
    "        'target_spacing': list(config['target_spacing']),\n",
    "        'target_size': list(config['target_size']),\n",
    "    },\n",
    "    'augmentation': {\n",
    "        'train': {},\n",
    "    },\n",
    "    'training': {\n",
    "        'batch_size': config['batch_size'],\n",
    "    },\n",
    "}\n",
    "\n",
    "dataloaders = get_dataloaders(\n",
    "    data_root=config['data_root'],\n",
    "    splits=splits,\n",
    "    config=dataloader_config,\n",
    "    dataset_name=config['dataset_name'],\n",
    "    num_workers=config['num_workers'],\n",
    ")\n",
    "\n",
    "print('\u2713 DataLoaders created')\n",
    "print('  Train batches: {}'.format(len(dataloaders['train'])))\n",
    "print('  Val batches: {}'.format(len(dataloaders['val'])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model\n",
    "\n",
    "Starting with a simple 3D U-Net for baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.networks.nets import UNet\n",
    "import torch.nn as nn\n",
    "\n",
    "# Create model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = UNet(\n",
    "    spatial_dims=3,\n",
    "    in_channels=1,  # Single MRI sequence\n",
    "    out_channels=config['num_classes'],\n",
    "    channels=(16, 32, 64, 128, 256),\n",
    "    strides=(2, 2, 2, 2),\n",
    "    num_res_units=2,\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\u2713 Model created\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"  Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.losses import DiceLoss\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# Loss function\n",
    "loss_fn = DiceLoss(\n",
    "    include_background=False,\n",
    "    to_onehot_y=True,\n",
    "    softmax=True\n",
    ")\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(\n",
    "    model.parameters(),\n",
    "    lr=config['learning_rate'],\n",
    "    weight_decay=config['weight_decay']\n",
    ")\n",
    "\n",
    "# Scheduler\n",
    "scheduler = CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=config['epochs'],\n",
    "    eta_min=1e-7\n",
    ")\n",
    "\n",
    "# Mixed precision\n",
    "scaler = GradScaler() if config['mixed_precision'] else None\n",
    "\n",
    "print(\"\u2713 Training setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Create checkpoint directory\n",
    "checkpoint_dir = Path(config['checkpoint_dir'])\n",
    "checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Check for existing checkpoints to resume\n",
    "latest_checkpoint = checkpoint_dir / 'latest.pth'\n",
    "start_epoch = 0\n",
    "best_dice = 0.0\n",
    "\n",
    "if latest_checkpoint.exists():\n",
    "    print(f\"Found checkpoint: {latest_checkpoint}\")\n",
    "    response = input(\"Resume from checkpoint? (y/n): \")\n",
    "    \n",
    "    if response.lower() == 'y':\n",
    "        checkpoint = torch.load(latest_checkpoint)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        best_dice = checkpoint.get('best_dice', 0.0)\n",
    "        \n",
    "        print(f\"\u2713 Resumed from epoch {start_epoch}\")\n",
    "        print(f\"  Best Dice: {best_dice:.4f}\")\n",
    "else:\n",
    "    print(\"No checkpoint found. Starting from scratch.\")\n",
    "\n",
    "print(f\"Starting training from epoch {start_epoch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "\n",
    "**Important for Colab:**\n",
    "- Saves checkpoints every 10 epochs\n",
    "- Can be interrupted and resumed\n",
    "- Progress saved to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, loss_fn, device, scaler=None):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in tqdm(dataloader, desc='Training'):\n",
    "        images = batch['image'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if scaler:\n",
    "            with autocast():\n",
    "                outputs = model(images)\n",
    "                loss = loss_fn(outputs, labels)\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            outputs = model(images)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def validate(model, dataloader, loss_fn, device):\n",
    "    \"\"\"Validate model\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc='Validation'):\n",
    "            images = batch['image'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "print(\"Training functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main training loop\n",
    "from datetime import datetime\n",
    "\n",
    "print(f\"Starting training at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Training for {config['epochs'] - start_epoch} epochs\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    for epoch in range(start_epoch, config['epochs']):\n",
    "        print(f\"\\nEpoch {epoch+1}/{config['epochs']}\")\n",
    "        \n",
    "        # Train\n",
    "        train_loss = train_epoch(\n",
    "            model, dataloaders['train'], optimizer, \n",
    "            loss_fn, device, scaler\n",
    "        )\n",
    "        \n",
    "        # Validate\n",
    "        val_loss = validate(\n",
    "            model, dataloaders['val'], loss_fn, device\n",
    "        )\n",
    "        \n",
    "        # Scheduler step\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Calculate Dice (1 - loss for DiceLoss)\n",
    "        val_dice = 1 - val_loss\n",
    "        \n",
    "        print(f\"Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"Val Loss: {val_loss:.4f} | Val Dice: {val_dice:.4f}\")\n",
    "        print(f\"LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "        \n",
    "        # Save checkpoint\n",
    "        if (epoch + 1) % config['save_frequency'] == 0 or val_dice > best_dice:\n",
    "            checkpoint = {\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'train_loss': train_loss,\n",
    "                'val_loss': val_loss,\n",
    "                'val_dice': val_dice,\n",
    "                'best_dice': max(best_dice, val_dice)\n",
    "            }\n",
    "            \n",
    "            # Save latest\n",
    "            torch.save(checkpoint, checkpoint_dir / 'latest.pth')\n",
    "            \n",
    "            # Save best\n",
    "            if val_dice > best_dice:\n",
    "                best_dice = val_dice\n",
    "                torch.save(checkpoint, checkpoint_dir / 'best.pth')\n",
    "                print(f\"\u2713 New best model! Dice: {best_dice:.4f}\")\n",
    "            \n",
    "            # Save periodic\n",
    "            if (epoch + 1) % config['save_frequency'] == 0:\n",
    "                torch.save(checkpoint, checkpoint_dir / f'epoch_{epoch+1}.pth')\n",
    "                print(f\"\u2713 Checkpoint saved (epoch {epoch+1})\")\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n\u26a0\ufe0f Training interrupted!\")\n",
    "    print(\"Checkpoint saved. You can resume later.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n\u274c Error during training: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "finally:\n",
    "    print(f\"\\nTraining session ended at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"Best Dice score: {best_dice:.4f}\")\n",
    "    print(f\"Checkpoints saved to: {checkpoint_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model and visualize predictions\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load best checkpoint\n",
    "best_checkpoint = torch.load(checkpoint_dir / 'best.pth')\n",
    "model.load_state_dict(best_checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "# Get a validation batch\n",
    "val_batch = next(iter(dataloaders['val']))\n",
    "images = val_batch['image'].to(device)\n",
    "labels = val_batch['label'].to(device)\n",
    "\n",
    "# Predict\n",
    "with torch.no_grad():\n",
    "    outputs = model(images)\n",
    "    preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "slice_idx = images.shape[-1] // 2\n",
    "\n",
    "axes[0].imshow(images[0, 0, :, :, slice_idx].cpu(), cmap='gray')\n",
    "axes[0].set_title('Input Image')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(labels[0, :, :, slice_idx].cpu(), cmap='jet')\n",
    "axes[1].set_title('Ground Truth')\n",
    "axes[1].axis('off')\n",
    "\n",
    "axes[2].imshow(preds[0, :, :, slice_idx].cpu(), cmap='jet')\n",
    "axes[2].set_title('Prediction')\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{PROJECT_ROOT}/experiments/results/prediction_sample.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\u2713 Visualization saved to {PROJECT_ROOT}/experiments/results/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Checkpoints (Optional)\n",
    "\n",
    "Download checkpoints to your local machine for backup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "# Download best model\n",
    "# files.download(str(checkpoint_dir / 'best.pth'))\n",
    "\n",
    "print(\"Uncomment to download checkpoints\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Complete!\n",
    "\n",
    "### Next Steps:\n",
    "1. **Evaluate on test set** - Use test_phase1.py or create evaluation notebook\n",
    "2. **Improve model** - Try Transformer architecture (Phase 3)\n",
    "3. **Add uncertainty** - Implement MC Dropout or Ensembles (Phase 4)\n",
    "4. **Analyze results** - Compare with paper baseline\n",
    "\n",
    "### Your checkpoints are saved in:\n",
    "```\n",
    "Google Drive/endo-seg/experiments/checkpoints/\n",
    "\u251c\u2500\u2500 best.pth      # Best model\n",
    "\u251c\u2500\u2500 latest.pth    # Latest checkpoint (for resuming)\n",
    "\u2514\u2500\u2500 epoch_*.pth   # Periodic checkpoints\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}