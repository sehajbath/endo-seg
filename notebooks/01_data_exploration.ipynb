{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UT-EndoMRI Dataset Exploration\n",
    "\n",
    "This notebook provides an exploratory analysis of the UT-EndoMRI dataset for endometriosis segmentation.\n",
    "\n",
    "**Contents:**\n",
    "1. Dataset Overview\n",
    "2. Load and Visualize MRI Scans\n",
    "3. Analyze Label Statistics\n",
    "4. Intensity Distribution Analysis\n",
    "5. Inter-rater Agreement (Dataset 1)\n",
    "6. Class Imbalance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "from src.data.utils import (\n",
    "    load_nifti,\n",
    "    get_dataset_statistics,\n",
    "    parse_filename,\n",
    "    get_subject_files,\n",
    "    load_data_splits\n",
    ")\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (15, 8)\n",
    "\n",
    "print(\"Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set paths\n",
    "data_root = \"../data/raw/UT-EndoMRI\"\n",
    "\n",
    "# Get statistics for both datasets\n",
    "print(\"Dataset 1 (D1_MHS - Multi-center, Multi-rater):\")\n",
    "print(\"=\"*60)\n",
    "stats_d1 = get_dataset_statistics(data_root, \"D1_MHS\")\n",
    "print(json.dumps(stats_d1, indent=2))\n",
    "\n",
    "print(\"\\n\\nDataset 2 (D2_TCPW - Single-center, Single-rater):\")\n",
    "print(\"=\"*60)\n",
    "stats_d2 = get_dataset_statistics(data_root, \"D2_TCPW\")\n",
    "print(json.dumps(stats_d2, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Visualize MRI Scans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a sample subject from Dataset 2\n",
    "subject_dir = Path(data_root) / \"D2_TCPW\" / \"D2-000\"\n",
    "files = get_subject_files(subject_dir)\n",
    "\n",
    "# Load T2FS image\n",
    "image_file = [f for f in files['images'] if 'T2FS' in f.name][0]\n",
    "image_data, image_obj = load_nifti(str(image_file))\n",
    "\n",
    "# Load labels\n",
    "label_files = {f.name.split('_')[1].replace('.nii.gz', ''): f for f in files['labels']}\n",
    "\n",
    "print(f\"Image shape: {image_data.shape}\")\n",
    "print(f\"Image spacing: {np.abs(np.diag(image_obj.affine)[:3])} mm\")\n",
    "print(f\"Intensity range: [{image_data.min():.2f}, {image_data.max():.2f}]\")\n",
    "print(f\"Available labels: {list(label_files.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize multiple slices\n",
    "def visualize_slices(image, label=None, num_slices=6, cmap='gray'):\n",
    "    \"\"\"Visualize multiple slices from 3D volume\"\"\"\n",
    "    depth = image.shape[2]\n",
    "    slice_indices = np.linspace(depth//4, 3*depth//4, num_slices, dtype=int)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, num_slices//2, figsize=(20, 8))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for idx, slice_idx in enumerate(slice_indices):\n",
    "        axes[idx].imshow(image[:, :, slice_idx].T, cmap=cmap, origin='lower')\n",
    "        \n",
    "        if label is not None:\n",
    "            # Overlay label\n",
    "            label_slice = label[:, :, slice_idx].T\n",
    "            masked = np.ma.masked_where(label_slice == 0, label_slice)\n",
    "            axes[idx].imshow(masked, cmap='jet', alpha=0.5, origin='lower')\n",
    "        \n",
    "        axes[idx].set_title(f'Slice {slice_idx}')\n",
    "        axes[idx].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Load uterus label\n",
    "if 'ut' in label_files:\n",
    "    ut_label, _ = load_nifti(str(label_files['ut']))\n",
    "    print(\"Visualizing T2FS image with uterus label overlay:\")\n",
    "    visualize_slices(image_data, ut_label)\n",
    "else:\n",
    "    print(\"Visualizing T2FS image only:\")\n",
    "    visualize_slices(image_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Label Statistics and Volume Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect volume statistics for all structures\n",
    "def collect_volumes(data_root, dataset_name):\n",
    "    \"\"\"Collect volume statistics for all structures\"\"\"\n",
    "    from collections import defaultdict\n",
    "    \n",
    "    dataset_path = Path(data_root) / dataset_name\n",
    "    volumes = defaultdict(list)\n",
    "    \n",
    "    for subject_dir in dataset_path.iterdir():\n",
    "        if not subject_dir.is_dir():\n",
    "            continue\n",
    "        \n",
    "        files = get_subject_files(subject_dir)\n",
    "        \n",
    "        for label_file in files['labels']:\n",
    "            info = parse_filename(label_file.name)\n",
    "            struct_type = info['type']\n",
    "            \n",
    "            try:\n",
    "                data, img = load_nifti(str(label_file))\n",
    "                spacing = np.abs(np.diag(img.affine)[:3])\n",
    "                voxel_volume = np.prod(spacing)\n",
    "                num_voxels = np.sum(data > 0)\n",
    "                volume_cc = (num_voxels * voxel_volume) / 1000\n",
    "                \n",
    "                if volume_cc > 0:\n",
    "                    volumes[struct_type].append(volume_cc)\n",
    "            except:\n",
    "                continue\n",
    "    \n",
    "    return volumes\n",
    "\n",
    "# Collect volumes\n",
    "print(\"Collecting volume statistics for Dataset 2...\")\n",
    "volumes_d2 = collect_volumes(data_root, \"D2_TCPW\")\n",
    "\n",
    "# Create DataFrame\n",
    "volume_data = []\n",
    "for struct, vols in volumes_d2.items():\n",
    "    for vol in vols:\n",
    "        volume_data.append({'Structure': struct, 'Volume (cc)': vol})\n",
    "\n",
    "df_volumes = pd.DataFrame(volume_data)\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\nVolume Statistics Summary:\")\n",
    "print(df_volumes.groupby('Structure')['Volume (cc)'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize volume distributions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Box plot\n",
    "df_volumes.boxplot(column='Volume (cc)', by='Structure', ax=axes[0])\n",
    "axes[0].set_title('Volume Distribution by Structure')\n",
    "axes[0].set_ylabel('Volume (cc)')\n",
    "axes[0].set_xlabel('Structure')\n",
    "plt.sca(axes[0])\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Violin plot\n",
    "sns.violinplot(data=df_volumes, x='Structure', y='Volume (cc)', ax=axes[1])\n",
    "axes[1].set_title('Volume Distribution (Violin Plot)')\n",
    "axes[1].set_xlabel('Structure')\n",
    "axes[1].set_ylabel('Volume (cc)')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print key insights\n",
    "print(\"\\nKey Insights:\")\n",
    "print(f\"- Uterus is the largest structure (mean: {df_volumes[df_volumes['Structure']=='ut']['Volume (cc)'].mean():.1f} cc)\")\n",
    "print(f\"- Ovaries are much smaller (mean: {df_volumes[df_volumes['Structure']=='ov']['Volume (cc)'].mean():.1f} cc)\")\n",
    "if 'em' in volumes_d2:\n",
    "    print(f\"- Endometriomas when present (mean: {df_volumes[df_volumes['Structure']=='em']['Volume (cc)'].mean():.1f} cc)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Intensity Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze intensity distributions within different structures\n",
    "def analyze_intensities(image, labels_dict):\n",
    "    \"\"\"Extract intensity statistics for each structure\"\"\"\n",
    "    intensities = {}\n",
    "    \n",
    "    for name, label in labels_dict.items():\n",
    "        mask = label > 0\n",
    "        if np.any(mask):\n",
    "            intensities[name] = image[mask]\n",
    "    \n",
    "    return intensities\n",
    "\n",
    "# Load multiple labels for current subject\n",
    "labels_dict = {}\n",
    "for name, file_path in label_files.items():\n",
    "    label_data, _ = load_nifti(str(file_path))\n",
    "    labels_dict[name] = label_data\n",
    "\n",
    "# Get intensities\n",
    "intensities = analyze_intensities(image_data, labels_dict)\n",
    "\n",
    "# Plot intensity distributions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Histogram\n",
    "for name, values in intensities.items():\n",
    "    axes[0].hist(values.flatten(), bins=50, alpha=0.6, label=name)\n",
    "axes[0].set_xlabel('Intensity Value')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Intensity Distribution by Structure')\n",
    "axes[0].legend()\n",
    "\n",
    "# Box plot\n",
    "intensity_data = []\n",
    "for name, values in intensities.items():\n",
    "    sample = np.random.choice(values.flatten(), min(1000, len(values.flatten())))\n",
    "    for val in sample:\n",
    "        intensity_data.append({'Structure': name, 'Intensity': val})\n",
    "\n",
    "df_intensity = pd.DataFrame(intensity_data)\n",
    "sns.boxplot(data=df_intensity, x='Structure', y='Intensity', ax=axes[1])\n",
    "axes[1].set_title('Intensity Distribution by Structure')\n",
    "axes[1].set_xlabel('Structure')\n",
    "axes[1].set_ylabel('Intensity')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print statistics\n",
    "print(\"\\nIntensity Statistics:\")\n",
    "for name, values in intensities.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Mean: {values.mean():.2f}\")\n",
    "    print(f\"  Std: {values.std():.2f}\")\n",
    "    print(f\"  Range: [{values.min():.2f}, {values.max():.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Class Imbalance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze class distribution in labels\n",
    "def compute_class_distribution(labels_dict):\n",
    "    \"\"\"Compute voxel counts for each structure\"\"\"\n",
    "    class_dist = {}\n",
    "    \n",
    "    # Get total volume size\n",
    "    total_voxels = list(labels_dict.values())[0].size\n",
    "    \n",
    "    # Count background (everything not labeled)\n",
    "    all_labels = np.zeros_like(list(labels_dict.values())[0])\n",
    "    for label in labels_dict.values():\n",
    "        all_labels = np.logical_or(all_labels, label > 0)\n",
    "    \n",
    "    class_dist['background'] = total_voxels - np.sum(all_labels)\n",
    "    \n",
    "    # Count each structure\n",
    "    for name, label in labels_dict.items():\n",
    "        class_dist[name] = np.sum(label > 0)\n",
    "    \n",
    "    return class_dist\n",
    "\n",
    "# Compute distribution\n",
    "class_dist = compute_class_distribution(labels_dict)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Pie chart\n",
    "axes[0].pie(class_dist.values(), labels=class_dist.keys(), autopct='%1.1f%%')\n",
    "axes[0].set_title('Class Distribution (Voxel Percentage)')\n",
    "\n",
    "# Bar chart (log scale)\n",
    "names = list(class_dist.keys())\n",
    "counts = list(class_dist.values())\n",
    "axes[1].bar(names, counts)\n",
    "axes[1].set_yscale('log')\n",
    "axes[1].set_xlabel('Class')\n",
    "axes[1].set_ylabel('Voxel Count (log scale)')\n",
    "axes[1].set_title('Class Distribution (Log Scale)')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print statistics\n",
    "print(\"\\nClass Imbalance Statistics:\")\n",
    "total = sum(class_dist.values())\n",
    "for name, count in class_dist.items():\n",
    "    percentage = (count / total) * 100\n",
    "    print(f\"{name}: {count} voxels ({percentage:.2f}%)\")\n",
    "\n",
    "# Compute imbalance ratio\n",
    "foreground_voxels = sum([v for k, v in class_dist.items() if k != 'background'])\n",
    "background_voxels = class_dist['background']\n",
    "imbalance_ratio = background_voxels / foreground_voxels\n",
    "print(f\"\\nBackground/Foreground ratio: {imbalance_ratio:.2f}:1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train/Val/Test Split Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load splits\n",
    "splits_file = \"../data/splits/split_info.json\"\n",
    "if Path(splits_file).exists():\n",
    "    splits = load_data_splits(splits_file)\n",
    "    \n",
    "    print(\"Data Split Information:\")\n",
    "    print(f\"\\nDataset: {splits['dataset']}\")\n",
    "    print(f\"Random seed: {splits['seed']}\")\n",
    "    print(f\"\\nSplit sizes:\")\n",
    "    print(f\"  Train: {len(splits['train'])} subjects ({splits['ratios']['train']:.1%})\")\n",
    "    print(f\"  Val: {len(splits['val'])} subjects ({splits['ratios']['val']:.1%})\")\n",
    "    print(f\"  Test: {len(splits['test'])} subjects ({splits['ratios']['test']:.1%})\")\n",
    "    \n",
    "    # Visualize\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    splits_counts = [len(splits['train']), len(splits['val']), len(splits['test'])]\n",
    "    ax.bar(['Train', 'Val', 'Test'], splits_counts, color=['#2ecc71', '#3498db', '#e74c3c'])\n",
    "    ax.set_ylabel('Number of Subjects')\n",
    "    ax.set_title('Train/Val/Test Split Distribution')\n",
    "    for i, v in enumerate(splits_counts):\n",
    "        ax.text(i, v + 0.5, str(v), ha='center', va='bottom')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Split file not found. Run: python scripts/create_splits.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Key Findings\n",
    "\n",
    "From this exploratory analysis, we can conclude:\n",
    "\n",
    "1. **Dataset Composition:**\n",
    "   - Dataset 1: Multi-center, multi-rater (more challenging)\n",
    "   - Dataset 2: Single-center, single-rater (more consistent)\n",
    "\n",
    "2. **Structure Sizes:**\n",
    "   - Uterus is the largest structure (~220 cc)\n",
    "   - Ovaries are much smaller (~12 cc)\n",
    "   - This size difference contributes to segmentation difficulty\n",
    "\n",
    "3. **Class Imbalance:**\n",
    "   - Significant imbalance between background and foreground\n",
    "   - Important for loss function design (Focal Loss, Tversky Loss)\n",
    "\n",
    "4. **Inter-rater Variability:**\n",
    "   - Lower agreement for ovaries vs uterus\n",
    "   - Justifies uncertainty quantification approach\n",
    "\n",
    "5. **Next Steps:**\n",
    "   - Implement preprocessing pipeline\n",
    "   - Design appropriate loss functions for imbalanced data\n",
    "   - Develop uncertainty-aware model architecture"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}