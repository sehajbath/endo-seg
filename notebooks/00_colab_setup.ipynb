{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Google Colab Setup for Endometriosis Segmentation\n",
    "\n",
    "This notebook sets up the complete environment on Google Colab.\n",
    "\n",
    "**Requirements:**\n",
    "- Google account\n",
    "- ~15GB Google Drive space (for dataset + checkpoints)\n",
    "- GPU runtime (free T4 or Colab Pro)\n",
    "\n",
    "**Runtime Settings:**\n",
    "1. Runtime ‚Üí Change runtime type\n",
    "2. Hardware accelerator: **GPU** (T4)\n",
    "3. Save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Check GPU and System Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi\n",
    "\n",
    "import torch\n",
    "print(f\"\\nPyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Mount Google Drive\n",
    "\n",
    "We'll store the dataset and checkpoints in Google Drive to persist across sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create project directory in Drive\n",
    "PROJECT_ROOT = '/content/drive/MyDrive/endometriosis-uncertainty-seg'\n",
    "os.makedirs(PROJECT_ROOT, exist_ok=True)\n",
    "\n",
    "print(f\"‚úì Project root: {PROJECT_ROOT}\")\n",
    "print(f\"‚úì Available space: {os.statvfs('/content/drive').f_bavail * os.statvfs('/content/drive').f_frsize / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Clone Repository\n",
    "\n",
    "Clone the project repository to the local Colab instance (faster access)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository to local Colab storage (faster)\n",
    "import os\n",
    "\n",
    "if not os.path.exists('/content/endo-uncertainty-seg'):\n",
    "    # Option 1: Clone from your GitHub (replace with your repo URL)\n",
    "    # !git clone https://github.com/yourusername/endo-uncertainty-seg.git /content/endo-uncertainty-seg\n",
    "    \n",
    "    # Option 2: Copy from Drive if you uploaded it\n",
    "    !cp -r {PROJECT_ROOT}/code /content/endo-uncertainty-seg\n",
    "    \n",
    "    # Option 3: Create fresh structure\n",
    "    !mkdir -p /content/endo-uncertainty-seg\n",
    "    print(\"‚úì Created project directory\")\n",
    "else:\n",
    "    print(\"‚úì Project directory already exists\")\n",
    "\n",
    "# Change to project directory\n",
    "%cd /content/endo-uncertainty-seg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install core dependencies\n",
    "!pip install -q monai nibabel SimpleITK pydicom\n",
    "!pip install -q pytorch-lightning wandb tensorboard\n",
    "!pip install -q scikit-image albumentations\n",
    "!pip install -q coloredlogs plotly\n",
    "\n",
    "print(\"‚úì Dependencies installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Setup Project Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory structure\n",
    "dirs = [\n",
    "    'src/data',\n",
    "    'src/models',\n",
    "    'src/training',\n",
    "    'src/utils',\n",
    "    'configs',\n",
    "    'scripts',\n",
    "    'notebooks',\n",
    "]\n",
    "\n",
    "for d in dirs:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "# Link data directory to Google Drive\n",
    "!ln -sf {PROJECT_ROOT}/data /content/endo-uncertainty-seg/data\n",
    "\n",
    "# Create experiment directories in Drive (persistent)\n",
    "!mkdir -p {PROJECT_ROOT}/experiments/checkpoints\n",
    "!mkdir -p {PROJECT_ROOT}/experiments/logs\n",
    "!mkdir -p {PROJECT_ROOT}/experiments/results\n",
    "!ln -sf {PROJECT_ROOT}/experiments /content/endo-uncertainty-seg/experiments\n",
    "\n",
    "print(\"‚úì Project structure created\")\n",
    "print(\"‚úì Data and experiments linked to Google Drive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Upload/Create Source Files\n",
    "\n",
    "**Option A:** Upload files from your local machine  \n",
    "**Option B:** Copy from GitHub  \n",
    "**Option C:** Create files directly in Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option A: Upload files\n",
    "from google.colab import files\n",
    "\n",
    "print(\"Upload your source files (.py files from src/)\")\n",
    "print(\"Or skip if using Option B/C\")\n",
    "\n",
    "# Uncomment to upload\n",
    "# uploaded = files.upload()\n",
    "\n",
    "# Option B: Clone from GitHub\n",
    "# Already done in Step 3\n",
    "\n",
    "# Option C: Will create minimal versions below\n",
    "print(\"‚úì Ready to create source files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Minimal Source Files (if not uploaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This creates minimal versions - replace with your full files\n",
    "# Or skip if you uploaded/cloned the full project\n",
    "\n",
    "import sys\n",
    "sys.path.append('/content/endo-uncertainty-seg')\n",
    "\n",
    "# Create __init__ files\n",
    "!touch src/__init__.py\n",
    "!touch src/data/__init__.py\n",
    "!touch src/models/__init__.py\n",
    "!touch src/utils/__init__.py\n",
    "\n",
    "print(\"‚úì Minimal structure created\")\n",
    "print(\"‚ö†Ô∏è  Upload your full source files for complete functionality\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Download Dataset to Google Drive\n",
    "\n",
    "**Important:** This is ~8GB and will take 15-30 minutes.  \n",
    "**Strategy:** Download once to Google Drive, reuse across sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if dataset already exists\n",
    "DATASET_PATH = f\"{PROJECT_ROOT}/data/raw/UT-EndoMRI\"\n",
    "\n",
    "if os.path.exists(DATASET_PATH):\n",
    "    print(\"‚úì Dataset already exists in Google Drive!\")\n",
    "    \n",
    "    # Verify dataset\n",
    "    d1_path = os.path.join(DATASET_PATH, \"D1_MHS\")\n",
    "    d2_path = os.path.join(DATASET_PATH, \"D2_TCPW\")\n",
    "    \n",
    "    if os.path.exists(d1_path):\n",
    "        d1_count = len([d for d in os.listdir(d1_path) if os.path.isdir(os.path.join(d1_path, d))])\n",
    "        print(f\"  Dataset 1: {d1_count} subjects\")\n",
    "    \n",
    "    if os.path.exists(d2_path):\n",
    "        d2_count = len([d for d in os.listdir(d2_path) if os.path.isdir(os.path.join(d2_path, d))])\n",
    "        print(f\"  Dataset 2: {d2_count} subjects\")\n",
    "else:\n",
    "    print(\"‚è≥ Downloading dataset... (this will take 15-30 minutes)\")\n",
    "    print(\"   You only need to do this ONCE - it will be saved to Drive\")\n",
    "    \n",
    "    # Create data directory\n",
    "    !mkdir -p {PROJECT_ROOT}/data/raw\n",
    "    \n",
    "    # Download dataset\n",
    "    !wget -O {PROJECT_ROOT}/data/raw/UT-EndoMRI.zip https://zenodo.org/records/15750762/files/UT-EndoMRI.zip\n",
    "    \n",
    "    print(\"‚úì Download complete!\")\n",
    "    print(\"‚è≥ Extracting...\")\n",
    "    \n",
    "    # Extract\n",
    "    !unzip -q {PROJECT_ROOT}/data/raw/UT-EndoMRI.zip -d {PROJECT_ROOT}/data/raw/\n",
    "    \n",
    "    print(\"‚úì Extraction complete!\")\n",
    "    \n",
    "    # Optional: Remove zip to save space\n",
    "    # !rm {PROJECT_ROOT}/data/raw/UT-EndoMRI.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Create Data Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create splits using paper's split or custom\n",
    "SPLITS_FILE = f\"{PROJECT_ROOT}/data/splits/split_info.json\"\n",
    "\n",
    "if os.path.exists(SPLITS_FILE):\n",
    "    print(\"‚úì Splits already exist!\")\n",
    "    \n",
    "    import json\n",
    "    with open(SPLITS_FILE, 'r') as f:\n",
    "        splits = json.load(f)\n",
    "    print(f\"  Train: {len(splits['train'])} subjects\")\n",
    "    print(f\"  Val: {len(splits['val'])} subjects\")\n",
    "    print(f\"  Test: {len(splits['test'])} subjects\")\n",
    "else:\n",
    "    print(\"Creating data splits...\")\n",
    "    \n",
    "    # You'll need to run create_splits.py here\n",
    "    # Or create splits manually\n",
    "    \n",
    "    import json\n",
    "    import numpy as np\n",
    "    \n",
    "    # Paper's split for D2_TCPW\n",
    "    train_val_ids = [f\"D2-{i:03d}\" for i in range(8)]\n",
    "    test_ids = [f\"D2-{i:03d}\" for i in range(8, 38)]\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    indices = np.random.permutation(len(train_val_ids))\n",
    "    n_train = int(len(train_val_ids) * 0.8)\n",
    "    \n",
    "    train_ids = [train_val_ids[i] for i in indices[:n_train]]\n",
    "    val_ids = [train_val_ids[i] for i in indices[n_train:]]\n",
    "    \n",
    "    splits = {\n",
    "        'train': train_ids,\n",
    "        'val': val_ids,\n",
    "        'test': test_ids,\n",
    "        'dataset': 'D2_TCPW',\n",
    "        'seed': 42\n",
    "    }\n",
    "    \n",
    "    os.makedirs(os.path.dirname(SPLITS_FILE), exist_ok=True)\n",
    "    with open(SPLITS_FILE, 'w') as f:\n",
    "        json.dump(splits, f, indent=2)\n",
    "    \n",
    "    print(\"‚úì Splits created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Test Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test that we can load data\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Load one sample\n",
    "dataset_path = Path(DATASET_PATH) / \"D2_TCPW\"\n",
    "sample_dir = list(dataset_path.glob(\"D2-*\"))[0]\n",
    "\n",
    "print(f\"Testing with: {sample_dir.name}\")\n",
    "\n",
    "# Find T2FS image\n",
    "t2fs_files = list(sample_dir.glob(\"*T2FS.nii.gz\"))\n",
    "if t2fs_files:\n",
    "    img = nib.load(str(t2fs_files[0]))\n",
    "    data = img.get_fdata()\n",
    "    \n",
    "    print(f\"‚úì Successfully loaded image\")\n",
    "    print(f\"  Shape: {data.shape}\")\n",
    "    print(f\"  Intensity range: [{data.min():.2f}, {data.max():.2f}]\")\n",
    "    print(f\"  Spacing: {np.abs(np.diag(img.affine)[:3])}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No T2FS image found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Quick Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Visualize middle slice\n",
    "if 'data' in locals():\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    # Axial\n",
    "    axes[0].imshow(data[:, :, data.shape[2]//2].T, cmap='gray', origin='lower')\n",
    "    axes[0].set_title('Axial View')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Sagittal\n",
    "    axes[1].imshow(data[data.shape[0]//2, :, :].T, cmap='gray', origin='lower')\n",
    "    axes[1].set_title('Sagittal View')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    # Coronal\n",
    "    axes[2].imshow(data[:, data.shape[1]//2, :].T, cmap='gray', origin='lower')\n",
    "    axes[2].set_title('Coronal View')\n",
    "    axes[2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"‚úì Visualization successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Setup Complete! üéâ\n",
    "\n",
    "### What's Ready:\n",
    "- ‚úÖ GPU environment configured\n",
    "- ‚úÖ Google Drive mounted\n",
    "- ‚úÖ Dependencies installed\n",
    "- ‚úÖ Dataset downloaded (8GB in Drive)\n",
    "- ‚úÖ Data splits created\n",
    "- ‚úÖ Project structure set up\n",
    "\n",
    "### Storage Layout:\n",
    "```\n",
    "Google Drive/endometriosis-uncertainty-seg/\n",
    "‚îú‚îÄ‚îÄ data/raw/UT-EndoMRI/     # 8GB dataset (persistent)\n",
    "‚îú‚îÄ‚îÄ data/splits/             # Train/val/test splits\n",
    "‚îú‚îÄ‚îÄ experiments/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ checkpoints/         # Model weights (persistent)\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ logs/               # Training logs\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ results/            # Predictions & metrics\n",
    "‚îî‚îÄ‚îÄ code/                    # Your source code (backup)\n",
    "```\n",
    "\n",
    "### Next Steps:\n",
    "1. **Continue to Phase 1:** Run data exploration\n",
    "2. **Start Phase 2:** Implement and train baseline model\n",
    "3. **Phase 3-4:** Implement Transformer + Uncertainty\n",
    "\n",
    "### üí° Colab Tips:\n",
    "- Save checkpoints frequently (to Drive)\n",
    "- Use smaller batch sizes (2-4) for T4 GPU\n",
    "- Consider Colab Pro for longer runtimes\n",
    "- Disconnect when not training to save compute quota"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Commands Reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save this cell for quick re-setup in future sessions\n",
    "\n",
    "# Mount Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Set paths\n",
    "PROJECT_ROOT = '/content/drive/MyDrive/endometriosis-uncertainty-seg'\n",
    "%cd /content/endo-uncertainty-seg\n",
    "\n",
    "# Link data and experiments\n",
    "!ln -sf {PROJECT_ROOT}/data /content/endo-uncertainty-seg/data\n",
    "!ln -sf {PROJECT_ROOT}/experiments /content/endo-uncertainty-seg/experiments\n",
    "\n",
    "import sys\n",
    "sys.path.append('/content/endo-uncertainty-seg')\n",
    "\n",
    "print(\"‚úì Quick setup complete!\")\n",
    "print(f\"‚úì GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}