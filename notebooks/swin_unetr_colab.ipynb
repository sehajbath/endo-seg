{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27c3edc5",
   "metadata": {},
   "source": [
    "# Swin UNETR Training with Uncertainty for UT-EndoMRI\n",
    "\n",
    "End-to-end Google Colab workflow for training a 3D Swin UNETR with Monte-Carlo dropout and test-time augmentation (TTA) uncertainty on the UT-EndoMRI dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8187e49",
   "metadata": {},
   "source": [
    "## 1. Environment & Paths\n",
    "Mount Google Drive, mirror the repository, install dependencies, and verify GPU/data availability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d63cdf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount(\"/content/drive\")\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "DRIVE_ROOT = \"/content/drive/MyDrive\"\n",
    "REPO_DIR = f\"{DRIVE_ROOT}/endo-seg\"\n",
    "WORK_DIR = \"/content/endo-seg\"\n",
    "DATASET_DIR = f\"{DRIVE_ROOT}/UT-EndoMRI\"\n",
    "\n",
    "os.environ[\"DRIVE_ROOT\"] = DRIVE_ROOT\n",
    "os.environ[\"REPO_DIR\"] = REPO_DIR\n",
    "os.environ[\"WORK_DIR\"] = WORK_DIR\n",
    "os.environ[\"DATASET_DIR\"] = DATASET_DIR\n",
    "\n",
    "print(f\"Drive root: {DRIVE_ROOT}\n",
    "Repo dir: {REPO_DIR}\n",
    "Work dir: {WORK_DIR}\n",
    "Dataset dir: {DATASET_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70dd32b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "if not os.path.isdir(REPO_DIR):\n",
    "    print(\"Cloning refactor-structure branch...\")\n",
    "    subprocess.run([\n",
    "        \"git\",\n",
    "        \"clone\",\n",
    "        \"-b\",\n",
    "        \"refactor-structure\",\n",
    "        \"https://github.com/sehajbath/endo-seg.git\",\n",
    "        REPO_DIR,\n",
    "    ], check=True)\n",
    "else:\n",
    "    print(\"Repository already present on Drive.\")\n",
    "\n",
    "subprocess.run([\"ln\", \"-sfn\", REPO_DIR, WORK_DIR], check=True)\n",
    "os.chdir(WORK_DIR)\n",
    "if os.path.join(WORK_DIR, \"src\") not in sys.path:\n",
    "    sys.path.append(os.path.join(WORK_DIR, \"src\"))\n",
    "\n",
    "%pip install -r requirements.txt\n",
    "%pip install -e .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0998bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "import torch\n",
    "\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"Total GPU memory (GB): {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f}\")\n",
    "\n",
    "assert os.path.isdir(DATASET_DIR), f\"Dataset directory not found: {DATASET_DIR}\"\n",
    "for split_dir in (\"D1_MHS\", \"D2_TCPW\"):\n",
    "    path = os.path.join(DATASET_DIR, split_dir)\n",
    "    assert os.path.isdir(path), f\"Missing dataset subset: {path}\"\n",
    "\n",
    "splits_file = os.path.join(REPO_DIR, \"data\", \"splits\", \"split_info.json\")\n",
    "os.makedirs(os.path.dirname(splits_file), exist_ok=True)\n",
    "if not os.path.isfile(splits_file):\n",
    "    print(\"Creating paper-aligned D2 split file...\")\n",
    "    subprocess.run([\n",
    "        \"python\",\n",
    "        \"scripts/create_splits.py\",\n",
    "        \"--data_root\",\n",
    "        DATASET_DIR,\n",
    "        \"--dataset\",\n",
    "        \"D2_TCPW\",\n",
    "        \"--output\",\n",
    "        splits_file,\n",
    "        \"--use_paper_split\",\n",
    "    ], check=True)\n",
    "else:\n",
    "    print(f\"Using existing splits file: {splits_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d47a4a3",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "Single source-of-truth for paths, datasets, model, training, and uncertainty parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586cdf95",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from pprint import pprint\n",
    "\n",
    "config = {\n",
    "    \"drive_root\": DRIVE_ROOT,\n",
    "    \"project_root\": REPO_DIR,\n",
    "    \"work_dir\": WORK_DIR,\n",
    "    \"dataset_dir\": DATASET_DIR,\n",
    "    \"splits_file\": splits_file,\n",
    "    \"checkpoint_dir\": os.path.join(REPO_DIR, \"experiments\", \"checkpoints_swin_unetr\"),\n",
    "    \"log_dir\": os.path.join(REPO_DIR, \"experiments\", \"logs_swin_unetr\"),\n",
    "    \"pretrain_dataset\": \"D2_TCPW\",\n",
    "    \"finetune_dataset\": \"D1_MHS\",\n",
    "    \"use_finetune_on_D1\": False,\n",
    "    \"sequences\": [\"T2FS\"],\n",
    "    \"structures\": [\"ut\", \"ov\", \"em\"],\n",
    "    \"num_classes\": 4,\n",
    "    \"target_spacing\": (5.0, 5.0, 5.0),\n",
    "    \"target_size\": (128, 128, 32),\n",
    "    \"intensity_clip_percentiles\": (1, 99),\n",
    "    \"epochs\": 150,\n",
    "    \"batch_size\": 2,\n",
    "    \"num_workers\": 4,\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"weight_decay\": 1e-5,\n",
    "    \"grad_clip\": 1.0,\n",
    "    \"mixed_precision\": True,\n",
    "    \"save_frequency\": 10,\n",
    "    \"feature_size\": 48,\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"attn_drop_rate\": 0.1,\n",
    "    \"dropout_path_rate\": 0.1,\n",
    "    \"use_checkpointing\": True,\n",
    "    \"sw_batch_size\": 2,\n",
    "    \"infer_overlap\": 0.5,\n",
    "    \"use_mc_dropout\": True,\n",
    "    \"num_mc_samples\": 8,\n",
    "    \"use_tta\": True,\n",
    "    \"num_tta_transforms\": 4,\n",
    "    \"seed\": 42,\n",
    "}\n",
    "\n",
    "\n",
    "def print_config(cfg):\n",
    "    pprint(cfg)\n",
    "\n",
    "print_config(config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7011fa59",
   "metadata": {},
   "source": [
    "## 3. Data Loading & Augmentation\n",
    "Reuse the project's dataset/preprocessing helpers and build PyTorch dataloaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e546c747",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "def set_seed(seed: int) -> None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "set_seed(config[\"seed\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c985948",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from endo_seg.data import EndoMRIDataset, MRIPreprocessor, load_data_splits\n",
    "from endo_seg.data.augment.transforms import get_train_transforms\n",
    "\n",
    "splits = load_data_splits(config[\"splits_file\"])\n",
    "print(f\"Train subjects: {len(splits['train'])}\")\n",
    "print(f\"Val subjects: {len(splits['val'])}\")\n",
    "print(f\"Test subjects: {len(splits['test'])}\")\n",
    "\n",
    "preprocessor = MRIPreprocessor(\n",
    "    target_spacing=config[\"target_spacing\"],\n",
    "    target_size=config[\"target_size\"],\n",
    "    intensity_clip_percentiles=config[\"intensity_clip_percentiles\"],\n",
    "    normalize_method=\"min_max\",\n",
    ")\n",
    "\n",
    "train_aug_cfg = {\n",
    "    \"random_flip_prob\": 0.5,\n",
    "    \"random_rotation\": 20,\n",
    "    \"random_translation\": 15,\n",
    "    \"random_elastic_deform\": True,\n",
    "    \"random_gamma\": [0.9, 1.1],\n",
    "    \"random_gaussian_noise\": 0.01,\n",
    "}\n",
    "train_transform = get_train_transforms(train_aug_cfg)\n",
    "\n",
    "train_dataset = EndoMRIDataset(\n",
    "    data_root=config[\"dataset_dir\"],\n",
    "    subject_ids=splits[\"train\"],\n",
    "    sequences=config[\"sequences\"],\n",
    "    structures=config[\"structures\"],\n",
    "    dataset_name=config[\"pretrain_dataset\"],\n",
    "    preprocessor=preprocessor,\n",
    "    transform=train_transform,\n",
    ")\n",
    "val_dataset = EndoMRIDataset(\n",
    "    data_root=config[\"dataset_dir\"],\n",
    "    subject_ids=splits[\"val\"],\n",
    "    sequences=config[\"sequences\"],\n",
    "    structures=config[\"structures\"],\n",
    "    dataset_name=config[\"pretrain_dataset\"],\n",
    "    preprocessor=preprocessor,\n",
    ")\n",
    "test_dataset = EndoMRIDataset(\n",
    "    data_root=config[\"dataset_dir\"],\n",
    "    subject_ids=splits[\"test\"],\n",
    "    sequences=config[\"sequences\"],\n",
    "    structures=config[\"structures\"],\n",
    "    dataset_name=config[\"pretrain_dataset\"],\n",
    "    preprocessor=preprocessor,\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config[\"batch_size\"],\n",
    "    shuffle=True,\n",
    "    num_workers=config[\"num_workers\"],\n",
    "    pin_memory=True,\n",
    "    drop_last=True,\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=config[\"num_workers\"],\n",
    "    pin_memory=True,\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=config[\"num_workers\"],\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "print(f\"Train batches per epoch: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5678f324",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch = next(iter(train_loader))\n",
    "print(\"Image batch shape:\", batch[\"image\"].shape)\n",
    "print(\"Label batch shape:\", batch[\"label\"].shape)\n",
    "print(\"Subject IDs example:\", batch.get(\"subject_id\", [])[:3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b3f353",
   "metadata": {},
   "source": [
    "## 4. Model & Trainer Setup\n",
    "Instantiate the Swin UNETR with uncertainty utilities and wire up the MONAI-style trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e2eba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from endo_seg.models.swin_unetr_uncertainty import SwinUNETRWithUncertainty\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = SwinUNETRWithUncertainty(\n",
    "    img_size=config[\"target_size\"],\n",
    "    in_channels=len(config[\"sequences\"]),\n",
    "    out_channels=config[\"num_classes\"],\n",
    "    feature_size=config[\"feature_size\"],\n",
    "    drop_rate=config[\"drop_rate\"],\n",
    "    attn_drop_rate=config[\"attn_drop_rate\"],\n",
    "    dropout_path_rate=config[\"dropout_path_rate\"],\n",
    "    use_checkpoint=config[\"use_checkpointing\"],\n",
    "    roi_size=config[\"target_size\"],\n",
    "    sw_batch_size=config[\"sw_batch_size\"],\n",
    "    infer_overlap=config[\"infer_overlap\"],\n",
    "    device=device,\n",
    ").to(device)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Model on {device} with {total_params/1e6:.2f}M trainable params\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6481b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from endo_seg.training.swin_unetr_trainer import train_loop\n",
    "\n",
    "history = train_loop(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    config=config,\n",
    "    device=device,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7142e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "axes[0].plot(history[\"train_loss\"], label=\"Train Loss\")\n",
    "axes[0].set_xlabel(\"Epoch\")\n",
    "axes[0].set_ylabel(\"Loss\")\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].plot(history[\"val_epochs\"], history[\"val_dice\"], marker=\"o\", label=\"Val Dice\")\n",
    "axes[1].set_xlabel(\"Epoch\")\n",
    "axes[1].set_ylabel(\"Mean Dice\")\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595a0fad",
   "metadata": {},
   "source": [
    "## 5. Uncertainty Evaluation & Visualization\n",
    "Load the best checkpoint, estimate predictive uncertainty with MC-dropout + TTA, and visualize entropy maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008645a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "best_ckpt = os.path.join(config[\"checkpoint_dir\"], \"best.pth\")\n",
    "if os.path.isfile(best_ckpt):\n",
    "    state = torch.load(best_ckpt, map_location=device)\n",
    "    model.load_state_dict(state[\"model_state\"])\n",
    "    print(f\"Loaded checkpoint from epoch {state['epoch']} with Dice {state['score']:.4f}\")\n",
    "else:\n",
    "    print(\"Warning: best checkpoint not found; using current model weights.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b6b509",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "model.eval()\n",
    "val_batch = next(iter(val_loader))\n",
    "inputs = val_batch[\"image\"].to(device)\n",
    "labels = val_batch[\"label\"].to(device)\n",
    "\n",
    "num_mc = config[\"num_mc_samples\"] if config[\"use_mc_dropout\"] else 1\n",
    "num_tta = config[\"num_tta_transforms\"] if config[\"use_tta\"] else 1\n",
    "\n",
    "with torch.no_grad():\n",
    "    mean_probs, pred_entropy, epistemic_var, aleatoric_var = model.predict_with_uncertainty(\n",
    "        inputs,\n",
    "        num_mc_samples=num_mc,\n",
    "        num_tta=num_tta,\n",
    "    )\n",
    "\n",
    "pred_labels = mean_probs.argmax(dim=1)\n",
    "\n",
    "# Per-class Dice\n",
    "per_class_dice = []\n",
    "for class_idx in range(config[\"num_classes\"]):\n",
    "    pred_mask = (pred_labels == class_idx).float()\n",
    "    true_mask = (labels == class_idx).float()\n",
    "    inter = (pred_mask * true_mask).sum()\n",
    "    denom = pred_mask.sum() + true_mask.sum()\n",
    "    dice = (2 * inter + 1e-5) / (denom + 1e-5)\n",
    "    per_class_dice.append(dice.item())\n",
    "\n",
    "print(\"Per-class Dice (background, uterus, ovary, endometrioma):\")\n",
    "print(per_class_dice)\n",
    "\n",
    "entropy_map = pred_entropy.squeeze(1)\n",
    "correct_mask = (pred_labels == labels)\n",
    "wrong_mask = ~correct_mask\n",
    "correct_entropy = entropy_map[correct_mask].mean().item()\n",
    "wrong_entropy = entropy_map[wrong_mask].mean().item()\n",
    "print(f\"Mean entropy -> correct voxels: {correct_entropy:.4f}, errors: {wrong_entropy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db21eb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img_np = inputs[0, 0].detach().cpu().numpy()\n",
    "label_np = labels[0].detach().cpu().numpy()\n",
    "pred_np = pred_labels[0].detach().cpu().numpy()\n",
    "entropy_np = pred_entropy[0, 0].detach().cpu().numpy()\n",
    "\n",
    "mid_slice = img_np.shape[-1] // 2\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(18, 4))\n",
    "axes[0].imshow(img_np[:, :, mid_slice], cmap=\"gray\")\n",
    "axes[0].set_title(\"T2FS slice\")\n",
    "\n",
    "axes[1].imshow(img_np[:, :, mid_slice], cmap=\"gray\")\n",
    "axes[1].imshow(np.ma.masked_where(label_np[:, :, mid_slice] == 0, label_np[:, :, mid_slice]), alpha=0.4)\n",
    "axes[1].set_title(\"Ground Truth\")\n",
    "\n",
    "axes[2].imshow(img_np[:, :, mid_slice], cmap=\"gray\")\n",
    "axes[2].imshow(np.ma.masked_where(pred_np[:, :, mid_slice] == 0, pred_np[:, :, mid_slice]), alpha=0.4)\n",
    "axes[2].set_title(\"Prediction\")\n",
    "\n",
    "entropy_slice = entropy_np[:, :, mid_slice]\n",
    "axes[3].imshow(entropy_slice, cmap=\"inferno\")\n",
    "axes[3].set_title(\"Predictive Entropy\")\n",
    "\n",
    "for ax in axes:\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "out_dir = os.path.join(config[\"project_root\"], \"experiments\", \"results_swin_unetr\")\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "fig_path = os.path.join(out_dir, \"sample_uncertainty_map.png\")\n",
    "plt.savefig(fig_path, dpi=150)\n",
    "print(f\"Saved visualization to {fig_path}\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500977ee",
   "metadata": {},
   "source": [
    "## 6. Next Steps\n",
    "- Toggle `config[\"use_finetune_on_D1\"] = True` and rebuild dataloaders for D1_MHS to fine-tune on heterogeneous data.\n",
    "- Train multiple seeds/checkpoints to build deep ensembles and combine with MC-dropout uncertainty for even stronger estimates.\n",
    "- Extend the visualization block to export NIfTI volumes or integrate with experiment tracking dashboards (W&B/TensorBoard)."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
