{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "27c3edc5",
      "metadata": {
        "id": "27c3edc5"
      },
      "source": [
        "# Swin UNETR Training with Uncertainty for UT-EndoMRI\n",
        "\n",
        "End-to-end Google Colab workflow for training a 3D Swin UNETR with Monte-Carlo dropout and test-time augmentation (TTA) uncertainty on the UT-EndoMRI dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8187e49",
      "metadata": {
        "id": "c8187e49"
      },
      "source": [
        "## 1. Environment & Paths\n",
        "Mount Google Drive, mirror the repository, install dependencies, and verify GPU/data availability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d63cdf3",
      "metadata": {
        "id": "2d63cdf3"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "\n",
        "DRIVE_ROOT = \"/content/drive/MyDrive\"\n",
        "REPO_DIR = f\"{DRIVE_ROOT}/endo-seg\"\n",
        "WORK_DIR = \"/content/endo-seg\"\n",
        "DATASET_DIR = f\"{DRIVE_ROOT}/UT-EndoMRI\"\n",
        "\n",
        "os.environ[\"DRIVE_ROOT\"] = DRIVE_ROOT\n",
        "os.environ[\"REPO_DIR\"] = REPO_DIR\n",
        "os.environ[\"WORK_DIR\"] = WORK_DIR\n",
        "os.environ[\"DATASET_DIR\"] = DATASET_DIR\n",
        "\n",
        "print(f\"Drive root: {DRIVE_ROOT}\\n\" \\\n",
        "      f\"Repo dir: {REPO_DIR}\\n\" \\\n",
        "      f\"Work dir: {WORK_DIR}\\n\" \\\n",
        "      f\"Dataset dir: {DATASET_DIR}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "70dd32b5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "collapsed": true,
        "id": "70dd32b5",
        "outputId": "76c95bbe-1607-4336-fb1e-1e4af5cc858f"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'REPO_DIR' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3097536641.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mREPO_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cloning refactor-structure branch...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     subprocess.run([\n",
            "\u001b[0;31mNameError\u001b[0m: name 'REPO_DIR' is not defined"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "if not os.path.isdir(REPO_DIR):\n",
        "    print(\"Cloning refactor-structure branch...\")\n",
        "    subprocess.run([\n",
        "        \"git\",\n",
        "        \"clone\",\n",
        "        \"-b\",\n",
        "        \"refactor-structure\",\n",
        "        \"https://github.com/sehajbath/endo-seg.git\",\n",
        "        REPO_DIR,\n",
        "    ], check=True)\n",
        "else:\n",
        "    print(\"Repository already present on Drive. Pulling latest updates...\")\n",
        "    subprocess.run([\"git\", \"-C\", REPO_DIR, \"pull\"], check=True)\n",
        "\n",
        "subprocess.run([\"ln\", \"-sfn\", REPO_DIR, WORK_DIR], check=True)\n",
        "os.chdir(WORK_DIR)\n",
        "if os.path.join(WORK_DIR, \"src\") not in sys.path:\n",
        "    sys.path.append(os.path.join(WORK_DIR, \"src\"))\n",
        "\n",
        "%pip install -r requirements.txt\n",
        "%pip install -e ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "f0998bc4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0998bc4",
        "outputId": "d8ea29f3-56a4-4acd-a31e-cdaec1904432"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU: NVIDIA A100-SXM4-40GB\n",
            "Total GPU memory (GB): 42.47\n",
            "Using existing splits file: /content/drive/MyDrive/endo-seg/data/splits/split_info.json\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import json\n",
        "import torch\n",
        "\n",
        "print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "print(f\"Total GPU memory (GB): {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f}\")\n",
        "\n",
        "assert os.path.isdir(DATASET_DIR), f\"Dataset directory not found: {DATASET_DIR}\"\n",
        "for split_dir in (\"D1_MHS\", \"D2_TCPW\"):\n",
        "    path = os.path.join(DATASET_DIR, split_dir)\n",
        "    assert os.path.isdir(path), f\"Missing dataset subset: {path}\"\n",
        "\n",
        "splits_file = os.path.join(REPO_DIR, \"data\", \"splits\", \"split_info.json\")\n",
        "os.makedirs(os.path.dirname(splits_file), exist_ok=True)\n",
        "if not os.path.isfile(splits_file):\n",
        "    print(\"Creating paper-aligned D2 split file...\")\n",
        "    subprocess.run([\n",
        "        \"python\",\n",
        "        \"scripts/create_splits.py\",\n",
        "        \"--data_root\",\n",
        "        DATASET_DIR,\n",
        "        \"--dataset\",\n",
        "        \"D2_TCPW\",\n",
        "        \"--output\",\n",
        "        splits_file,\n",
        "        \"--use_paper_split\",\n",
        "    ], check=True)\n",
        "else:\n",
        "    print(f\"Using existing splits file: {splits_file}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d47a4a3",
      "metadata": {
        "id": "9d47a4a3"
      },
      "source": [
        "## 2. Configuration\n",
        "Single source-of-truth for paths, datasets, model, training, and uncertainty parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "586cdf95",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "586cdf95",
        "outputId": "b6fd831e-1d21-442f-b4e9-0596820900e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'attn_drop_rate': 0.1,\n",
            " 'batch_size': 2,\n",
            " 'checkpoint_dir': '/content/drive/MyDrive/endo-seg/experiments/checkpoints_swin_unetr',\n",
            " 'dataset_dir': '/content/drive/MyDrive/UT-EndoMRI',\n",
            " 'drive_root': '/content/drive/MyDrive',\n",
            " 'drop_rate': 0.1,\n",
            " 'dropout_path_rate': 0.1,\n",
            " 'epochs': 150,\n",
            " 'feature_size': 48,\n",
            " 'finetune_dataset': 'D1_MHS',\n",
            " 'grad_clip': 1.0,\n",
            " 'infer_overlap': 0.5,\n",
            " 'intensity_clip_percentiles': (1, 99),\n",
            " 'learning_rate': 0.0001,\n",
            " 'log_dir': '/content/drive/MyDrive/endo-seg/experiments/logs_swin_unetr',\n",
            " 'mixed_precision': True,\n",
            " 'num_classes': 4,\n",
            " 'num_mc_samples': 8,\n",
            " 'num_tta_transforms': 4,\n",
            " 'num_workers': 4,\n",
            " 'pretrain_dataset': 'D2_TCPW',\n",
            " 'project_root': '/content/drive/MyDrive/endo-seg',\n",
            " 'save_frequency': 10,\n",
            " 'seed': 42,\n",
            " 'sequences': ['T2FS'],\n",
            " 'splits_file': '/content/drive/MyDrive/endo-seg/data/splits/split_info.json',\n",
            " 'structures': ['ut', 'ov', 'em'],\n",
            " 'sw_batch_size': 2,\n",
            " 'target_size': (128, 128, 32),\n",
            " 'target_spacing': (5.0, 5.0, 5.0),\n",
            " 'use_checkpointing': True,\n",
            " 'use_finetune_on_D1': False,\n",
            " 'use_mc_dropout': True,\n",
            " 'use_tta': True,\n",
            " 'weight_decay': 1e-05,\n",
            " 'work_dir': '/content/endo-seg'}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import os\n",
        "from pprint import pprint\n",
        "\n",
        "config = {\n",
        "    \"drive_root\": DRIVE_ROOT,\n",
        "    \"project_root\": REPO_DIR,\n",
        "    \"work_dir\": WORK_DIR,\n",
        "    \"dataset_dir\": DATASET_DIR,\n",
        "    \"splits_file\": splits_file,\n",
        "    \"checkpoint_dir\": os.path.join(REPO_DIR, \"experiments\", \"checkpoints_swin_unetr\"),\n",
        "    \"log_dir\": os.path.join(REPO_DIR, \"experiments\", \"logs_swin_unetr\"),\n",
        "    \"pretrain_dataset\": \"D2_TCPW\",\n",
        "    \"finetune_dataset\": \"D1_MHS\",\n",
        "    \"use_finetune_on_D1\": False,\n",
        "    \"sequences\": [\"T2FS\"],\n",
        "    \"structures\": [\"ut\", \"ov\", \"em\"],\n",
        "    \"num_classes\": 4,\n",
        "    \"target_spacing\": (5.0, 5.0, 5.0),\n",
        "    \"target_size\": (128, 128, 32),\n",
        "    \"intensity_clip_percentiles\": (1, 99),\n",
        "    \"epochs\": 150,\n",
        "    \"batch_size\": 2,\n",
        "    \"num_workers\": 4,\n",
        "    \"learning_rate\": 1e-4,\n",
        "    \"weight_decay\": 1e-5,\n",
        "    \"grad_clip\": 1.0,\n",
        "    \"mixed_precision\": True,\n",
        "    \"save_frequency\": 10,\n",
        "    \"feature_size\": 48,\n",
        "    \"drop_rate\": 0.1,\n",
        "    \"attn_drop_rate\": 0.1,\n",
        "    \"dropout_path_rate\": 0.1,\n",
        "    \"use_checkpointing\": True,\n",
        "    \"sw_batch_size\": 2,\n",
        "    \"infer_overlap\": 0.5,\n",
        "    \"use_mc_dropout\": True,\n",
        "    \"num_mc_samples\": 8,\n",
        "    \"use_tta\": True,\n",
        "    \"num_tta_transforms\": 4,\n",
        "    \"seed\": 42,\n",
        "}\n",
        "\n",
        "\n",
        "def print_config(cfg):\n",
        "    pprint(cfg)\n",
        "\n",
        "print_config(config)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7011fa59",
      "metadata": {
        "id": "7011fa59"
      },
      "source": [
        "## 3. Data Loading & Augmentation\n",
        "Reuse the project's dataset/preprocessing helpers and build PyTorch dataloaders."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "e546c747",
      "metadata": {
        "id": "e546c747"
      },
      "outputs": [],
      "source": [
        "\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "\n",
        "def set_seed(seed: int) -> None:\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "\n",
        "set_seed(config[\"seed\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "5c985948",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5c985948",
        "outputId": "dad10732-eda3-4935-a1ee-caf034b9feb5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:endo_seg.data.core.dataset:Subject directory not found: /content/drive/MyDrive/UT-EndoMRI/D2_TCPW/D2-030\n",
            "WARNING:endo_seg.data.core.dataset:Subject directory not found: /content/drive/MyDrive/UT-EndoMRI/D2_TCPW/D2-031\n",
            "WARNING:endo_seg.data.core.dataset:Subject directory not found: /content/drive/MyDrive/UT-EndoMRI/D2_TCPW/D2-032\n",
            "WARNING:endo_seg.data.core.dataset:Subject directory not found: /content/drive/MyDrive/UT-EndoMRI/D2_TCPW/D2-033\n",
            "WARNING:endo_seg.data.core.dataset:Subject directory not found: /content/drive/MyDrive/UT-EndoMRI/D2_TCPW/D2-034\n",
            "WARNING:endo_seg.data.core.dataset:Subject directory not found: /content/drive/MyDrive/UT-EndoMRI/D2_TCPW/D2-035\n",
            "WARNING:endo_seg.data.core.dataset:Subject directory not found: /content/drive/MyDrive/UT-EndoMRI/D2_TCPW/D2-036\n",
            "WARNING:endo_seg.data.core.dataset:Subject directory not found: /content/drive/MyDrive/UT-EndoMRI/D2_TCPW/D2-037\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train subjects: 6\n",
            "Val subjects: 2\n",
            "Test subjects: 30\n",
            "Train batches per epoch: 3\n",
            "Val batches: 2\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from endo_seg.data import EndoMRIDataset, MRIPreprocessor, load_data_splits\n",
        "from endo_seg.data.augment.transforms import get_train_transforms\n",
        "\n",
        "splits = load_data_splits(config[\"splits_file\"])\n",
        "print(f\"Train subjects: {len(splits['train'])}\")\n",
        "print(f\"Val subjects: {len(splits['val'])}\")\n",
        "print(f\"Test subjects: {len(splits['test'])}\")\n",
        "\n",
        "preprocessor = MRIPreprocessor(\n",
        "    target_spacing=config[\"target_spacing\"],\n",
        "    target_size=config[\"target_size\"],\n",
        "    intensity_clip_percentiles=config[\"intensity_clip_percentiles\"],\n",
        "    normalize_method=\"min_max\",\n",
        ")\n",
        "\n",
        "train_aug_cfg = {\n",
        "    \"random_flip_prob\": 0.5,\n",
        "    \"random_rotation\": 20,\n",
        "    \"random_translation\": 15,\n",
        "    \"random_elastic_deform\": True,\n",
        "    \"random_gamma\": [0.9, 1.1],\n",
        "    \"random_gaussian_noise\": 0.01,\n",
        "}\n",
        "train_transform = get_train_transforms(train_aug_cfg)\n",
        "\n",
        "train_dataset = EndoMRIDataset(\n",
        "    data_root=config[\"dataset_dir\"],\n",
        "    subject_ids=splits[\"train\"],\n",
        "    sequences=config[\"sequences\"],\n",
        "    structures=config[\"structures\"],\n",
        "    dataset_name=config[\"pretrain_dataset\"],\n",
        "    preprocessor=preprocessor,\n",
        "    transform=train_transform,\n",
        ")\n",
        "val_dataset = EndoMRIDataset(\n",
        "    data_root=config[\"dataset_dir\"],\n",
        "    subject_ids=splits[\"val\"],\n",
        "    sequences=config[\"sequences\"],\n",
        "    structures=config[\"structures\"],\n",
        "    dataset_name=config[\"pretrain_dataset\"],\n",
        "    preprocessor=preprocessor,\n",
        ")\n",
        "test_dataset = EndoMRIDataset(\n",
        "    data_root=config[\"dataset_dir\"],\n",
        "    subject_ids=splits[\"test\"],\n",
        "    sequences=config[\"sequences\"],\n",
        "    structures=config[\"structures\"],\n",
        "    dataset_name=config[\"pretrain_dataset\"],\n",
        "    preprocessor=preprocessor,\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=config[\"batch_size\"],\n",
        "    shuffle=True,\n",
        "    num_workers=config[\"num_workers\"],\n",
        "    pin_memory=True,\n",
        "    drop_last=True,\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=1,\n",
        "    shuffle=False,\n",
        "    num_workers=config[\"num_workers\"],\n",
        "    pin_memory=True,\n",
        ")\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=1,\n",
        "    shuffle=False,\n",
        "    num_workers=config[\"num_workers\"],\n",
        "    pin_memory=True,\n",
        ")\n",
        "\n",
        "print(f\"Train batches per epoch: {len(train_loader)}\")\n",
        "print(f\"Val batches: {len(val_loader)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "5678f324",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5678f324",
        "outputId": "560e7e9e-7e00-4e2c-f598-76b280e09808"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image batch shape: torch.Size([2, 1, 128, 128, 32])\n",
            "Label batch shape: torch.Size([2, 128, 128, 32])\n",
            "Subject IDs example: ['D2-002', 'D2-004']\n"
          ]
        }
      ],
      "source": [
        "\n",
        "batch = next(iter(train_loader))\n",
        "print(\"Image batch shape:\", batch[\"image\"].shape)\n",
        "print(\"Label batch shape:\", batch[\"label\"].shape)\n",
        "print(\"Subject IDs example:\", batch.get(\"subject_id\", [])[:3])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0b3f353",
      "metadata": {
        "id": "a0b3f353"
      },
      "source": [
        "## 4. Model & Trainer Setup\n",
        "Instantiate the Swin UNETR with uncertainty utilities and wire up the MONAI-style trainer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "e3e2eba7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3e2eba7",
        "outputId": "25cde681-684c-469f-f49e-cf1952de2e47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<frozen importlib._bootstrap_external>:1301: FutureWarning: The cuda.cudart module is deprecated and will be removed in a future release, please switch to use the cuda.bindings.runtime module instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model on cuda with 62.19M trainable params\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import torch\n",
        "from endo_seg.models.swin_unetr_uncertainty import SwinUNETRWithUncertainty\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = SwinUNETRWithUncertainty(\n",
        "    #img_size=config[\"target_size\"],\n",
        "    in_channels=len(config[\"sequences\"]),\n",
        "    out_channels=config[\"num_classes\"],\n",
        "    feature_size=config[\"feature_size\"],\n",
        "    drop_rate=config[\"drop_rate\"],\n",
        "    attn_drop_rate=config[\"attn_drop_rate\"],\n",
        "    dropout_path_rate=config[\"dropout_path_rate\"],\n",
        "    use_checkpoint=config[\"use_checkpointing\"],\n",
        "    roi_size=config[\"target_size\"],\n",
        "    sw_batch_size=config[\"sw_batch_size\"],\n",
        "    infer_overlap=config[\"infer_overlap\"],\n",
        "    device=device,\n",
        ").to(device)\n",
        "\n",
        "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Model on {device} with {total_params/1e6:.2f}M trainable params\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "f6481b86",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 523
        },
        "id": "f6481b86",
        "outputId": "823b77d2-4c24-48df-c52d-69ef697c18da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/drive/MyDrive/endo-seg/src/endo_seg/training/swin_unetr_trainer.py:187: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=mixed_precision)\n",
            "/content/drive/MyDrive/endo-seg/src/endo_seg/training/swin_unetr_trainer.py:86: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=mixed_precision):\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "the number of dimensions for input and target should be the same, got shape torch.Size([2, 4, 128, 128, 32]) (nb dims: 5) and torch.Size([2, 128, 128, 32]) (nb dims: 4). if target is not one-hot encoded, please provide a tensor with shape B1H[WD].",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2927193061.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mendo_seg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswin_unetr_trainer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m history = train_loop(\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtrain_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/endo-seg/src/endo_seg/training/swin_unetr_trainer.py\u001b[0m in \u001b[0;36mtrain_loop\u001b[0;34m(model, train_loader, val_loader, config, device)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m         train_loss = train_one_epoch(\n\u001b[0m\u001b[1;32m    196\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m             \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/endo-seg/src/endo_seg/training/swin_unetr_trainer.py\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, loader, optimizer, loss_fn, device, epoch, max_epochs, grad_clip, mixed_precision, scaler)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmixed_precision\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/monai/losses/dice.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    789\u001b[0m         \"\"\"\n\u001b[1;32m    790\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 791\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    792\u001b[0m                 \u001b[0;34m\"the number of dimensions for input and target should be the same, \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m                 \u001b[0;34mf\"got shape {input.shape} (nb dims: {len(input.shape)}) and {target.shape} (nb dims: {len(target.shape)}). \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: the number of dimensions for input and target should be the same, got shape torch.Size([2, 4, 128, 128, 32]) (nb dims: 5) and torch.Size([2, 128, 128, 32]) (nb dims: 4). if target is not one-hot encoded, please provide a tensor with shape B1H[WD]."
          ]
        }
      ],
      "source": [
        "\n",
        "from endo_seg.training.swin_unetr_trainer import train_loop\n",
        "\n",
        "history = train_loop(\n",
        "    model=model,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    config=config,\n",
        "    device=device,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a7142e2",
      "metadata": {
        "id": "5a7142e2"
      },
      "outputs": [],
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "axes[0].plot(history[\"train_loss\"], label=\"Train Loss\")\n",
        "axes[0].set_xlabel(\"Epoch\")\n",
        "axes[0].set_ylabel(\"Loss\")\n",
        "axes[0].legend()\n",
        "\n",
        "axes[1].plot(history[\"val_epochs\"], history[\"val_dice\"], marker=\"o\", label=\"Val Dice\")\n",
        "axes[1].set_xlabel(\"Epoch\")\n",
        "axes[1].set_ylabel(\"Mean Dice\")\n",
        "axes[1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "595a0fad",
      "metadata": {
        "id": "595a0fad"
      },
      "source": [
        "## 5. Uncertainty Evaluation & Visualization\n",
        "Load the best checkpoint, estimate predictive uncertainty with MC-dropout + TTA, and visualize entropy maps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "008645a3",
      "metadata": {
        "id": "008645a3"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "\n",
        "best_ckpt = os.path.join(config[\"checkpoint_dir\"], \"best.pth\")\n",
        "if os.path.isfile(best_ckpt):\n",
        "    state = torch.load(best_ckpt, map_location=device)\n",
        "    model.load_state_dict(state[\"model_state\"])\n",
        "    print(f\"Loaded checkpoint from epoch {state['epoch']} with Dice {state['score']:.4f}\")\n",
        "else:\n",
        "    print(\"Warning: best checkpoint not found; using current model weights.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32b6b509",
      "metadata": {
        "id": "32b6b509"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "model.eval()\n",
        "val_batch = next(iter(val_loader))\n",
        "inputs = val_batch[\"image\"].to(device)\n",
        "labels = val_batch[\"label\"].to(device)\n",
        "\n",
        "num_mc = config[\"num_mc_samples\"] if config[\"use_mc_dropout\"] else 1\n",
        "num_tta = config[\"num_tta_transforms\"] if config[\"use_tta\"] else 1\n",
        "\n",
        "with torch.no_grad():\n",
        "    mean_probs, pred_entropy, epistemic_var, aleatoric_var = model.predict_with_uncertainty(\n",
        "        inputs,\n",
        "        num_mc_samples=num_mc,\n",
        "        num_tta=num_tta,\n",
        "    )\n",
        "\n",
        "pred_labels = mean_probs.argmax(dim=1)\n",
        "\n",
        "# Per-class Dice\n",
        "per_class_dice = []\n",
        "for class_idx in range(config[\"num_classes\"]):\n",
        "    pred_mask = (pred_labels == class_idx).float()\n",
        "    true_mask = (labels == class_idx).float()\n",
        "    inter = (pred_mask * true_mask).sum()\n",
        "    denom = pred_mask.sum() + true_mask.sum()\n",
        "    dice = (2 * inter + 1e-5) / (denom + 1e-5)\n",
        "    per_class_dice.append(dice.item())\n",
        "\n",
        "print(\"Per-class Dice (background, uterus, ovary, endometrioma):\")\n",
        "print(per_class_dice)\n",
        "\n",
        "entropy_map = pred_entropy.squeeze(1)\n",
        "correct_mask = (pred_labels == labels)\n",
        "wrong_mask = ~correct_mask\n",
        "correct_entropy = entropy_map[correct_mask].mean().item()\n",
        "wrong_entropy = entropy_map[wrong_mask].mean().item()\n",
        "print(f\"Mean entropy -> correct voxels: {correct_entropy:.4f}, errors: {wrong_entropy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db21eb56",
      "metadata": {
        "id": "db21eb56"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "img_np = inputs[0, 0].detach().cpu().numpy()\n",
        "label_np = labels[0].detach().cpu().numpy()\n",
        "pred_np = pred_labels[0].detach().cpu().numpy()\n",
        "entropy_np = pred_entropy[0, 0].detach().cpu().numpy()\n",
        "\n",
        "mid_slice = img_np.shape[-1] // 2\n",
        "\n",
        "fig, axes = plt.subplots(1, 4, figsize=(18, 4))\n",
        "axes[0].imshow(img_np[:, :, mid_slice], cmap=\"gray\")\n",
        "axes[0].set_title(\"T2FS slice\")\n",
        "\n",
        "axes[1].imshow(img_np[:, :, mid_slice], cmap=\"gray\")\n",
        "axes[1].imshow(np.ma.masked_where(label_np[:, :, mid_slice] == 0, label_np[:, :, mid_slice]), alpha=0.4)\n",
        "axes[1].set_title(\"Ground Truth\")\n",
        "\n",
        "axes[2].imshow(img_np[:, :, mid_slice], cmap=\"gray\")\n",
        "axes[2].imshow(np.ma.masked_where(pred_np[:, :, mid_slice] == 0, pred_np[:, :, mid_slice]), alpha=0.4)\n",
        "axes[2].set_title(\"Prediction\")\n",
        "\n",
        "entropy_slice = entropy_np[:, :, mid_slice]\n",
        "axes[3].imshow(entropy_slice, cmap=\"inferno\")\n",
        "axes[3].set_title(\"Predictive Entropy\")\n",
        "\n",
        "for ax in axes:\n",
        "    ax.axis(\"off\")\n",
        "\n",
        "plt.tight_layout()\n",
        "out_dir = os.path.join(config[\"project_root\"], \"experiments\", \"results_swin_unetr\")\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "fig_path = os.path.join(out_dir, \"sample_uncertainty_map.png\")\n",
        "plt.savefig(fig_path, dpi=150)\n",
        "print(f\"Saved visualization to {fig_path}\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "500977ee",
      "metadata": {
        "id": "500977ee"
      },
      "source": [
        "## 6. Next Steps\n",
        "- Toggle `config[\"use_finetune_on_D1\"] = True` and rebuild dataloaders for D1_MHS to fine-tune on heterogeneous data.\n",
        "- Train multiple seeds/checkpoints to build deep ensembles and combine with MC-dropout uncertainty for even stronger estimates.\n",
        "- Extend the visualization block to export NIfTI volumes or integrate with experiment tracking dashboards (W&B/TensorBoard)."
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}